{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9b4b85-9d17-468c-b244-73a1c5191e3e",
   "metadata": {},
   "source": [
    "# RAG Avancé: HQ (Hypothetical Question),Chromadb, BM25, et Gemini "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0659b3",
   "metadata": {},
   "source": [
    "#### Bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3bd8048f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#API key:\n",
    "# Head to https://ai.google.dev/gemini-api/docs/api-key to generate a Google AI API key. Paste in .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5658a",
   "metadata": {},
   "source": [
    "### Étape 1 : Définir la structure de données avec Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "16a7870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentData(BaseModel):\n",
    "    title: Optional[str]\n",
    "    author: Optional[str]\n",
    "    content: str\n",
    "    metadata: Optional[dict]\n",
    "    source: str\n",
    "    page_number: int\n",
    "    excerpt: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f237f39e",
   "metadata": {},
   "source": [
    "#### Étape 2 : Charger et extraire les données du/des PDF(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4d311397",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"data/doc.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "deed3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrait de texte autour de la réponse\n",
    "def get_excerpt(page_content: str, query: str, window_size: int = 100):\n",
    "    \"\"\"Retourne un extrait de texte autour de la question\"\"\"\n",
    "    idx = page_content.find(query)\n",
    "    if idx == -1:\n",
    "        return page_content[:window_size] #\n",
    "    start_idx = max(idx - window_size, 0)\n",
    "    end_idx = min(idx + len(query) + window_size, len(page_content))\n",
    "    return page_content[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5c91bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecte des données structurées pour chaque page du PDF\n",
    "documents_data = []\n",
    "for page_number, page in enumerate(data, 1):\n",
    "    document_content = page.page_content\n",
    "    excerpt = get_excerpt(document_content, query) \n",
    "    metadata = {\"source\": \"doc.pdf\", \"page\": page_number}\n",
    "\n",
    "    document_data = DocumentData(\n",
    "        title=\"Exemple de Titre\", \n",
    "        author=\"Nom de l'Auteur\", \n",
    "        content=document_content,\n",
    "        metadata=metadata,\n",
    "        source=\"doc.pdf\", \n",
    "        page_number=page_number,\n",
    "        excerpt=excerpt \n",
    "    )\n",
    "    documents_data.append(document_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12c8d3",
   "metadata": {},
   "source": [
    "#### Split & Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0c7eb061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0031878091394901276,\n",
       " -0.03396224603056908,\n",
       " -0.04738517105579376,\n",
       " -0.02076413296163082,\n",
       " 0.04006507992744446]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "# Embedding models: https://python.langchain.com/v0.1/docs/integrations/text_embedding/\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"chromadb\")\n",
    "\n",
    "#vector\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e103b86",
   "metadata": {},
   "source": [
    "#### HQ & Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8a507c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"comment se passe la fourniture d’un complément de liquide de refroidissement \")\n",
    "\n",
    "#ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0.3, max_tokens=500)\n",
    "\n",
    "#BM25\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "# Création de l'EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vectorstore.as_retriever()],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "#Fonction questions hypothétiques\n",
    "def generate_hypothetical_questions(query, llm):\n",
    "    prompt = f\"Générez 3 questions hypothétiques liées à cette requête : {query}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d65c87",
   "metadata": {},
   "source": [
    "#### Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "671796e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    '''Tu es un assistant technique ton ôle est de répondre aux questions.\n",
    "    Utilise les éléments de contexte suivants pour répondre aux questions\n",
    "    Si tu ne connais pas la réponse, dites je ne connais pas. Utilise trois\n",
    "    phrases au maximum et faites en sorte que la réponse soit concise.'''\n",
    "\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd383615",
   "metadata": {},
   "source": [
    "#### Chaine de RAG avancée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a822fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def advanced_rag_chain(query):\n",
    "    hq_questions = generate_hypothetical_questions(query, llm)  # Génération de questions hypothétiques\n",
    "    \n",
    "    all_docs = ensemble_retriever.invoke(query)  # Recherche avec la requête originale et les questions hypothétiques\n",
    "    for hq in hq_questions:\n",
    "        all_docs.extend(ensemble_retriever.invoke(hq))\n",
    "    \n",
    "    # Déduplication et limitation du nombre de documents\n",
    "    unique_docs = list({doc.metadata['source'] + str(doc.metadata['page']): doc for doc in all_docs}.values())\n",
    "    context_docs = unique_docs[:10] \n",
    "\n",
    "    # Génération de la réponse\n",
    "    response = rag_chain.invoke({\"input\": query, \"context\": context_docs})\n",
    "\n",
    "    # Extraction des informations de source\n",
    "    if context_docs:\n",
    "        source_info = context_docs[0].metadata\n",
    "        source_name = source_info.get('source', 'N/A')\n",
    "        page_number = source_info.get('page', 'N/A')\n",
    "        excerpt = context_docs[0].page_content[:200] + \"...\"\n",
    "\n",
    "        # Ajout des informations sur la source\n",
    "        source_string = f\"Source: {source_name}\\nPage: {page_number}\\nExtrait: {excerpt}\"\n",
    "    else:\n",
    "        source_string = \"Aucune source trouvée.\"\n",
    "\n",
    "    # Affichage des questions hypothétiques\n",
    "    hq_display = \"\\n\".join(hq_questions)\n",
    "    output = f\"Questions hypothétiques générées :\\n{hq_display}\\n\\nRéponse : {response['answer']}\\n\\n{source_string}\"\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f813a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions hypothétiques générées :\n",
      "1. Si un système de climatisation d'une locomotive tombe en panne en plein été, quelles opérations de maintenance corrective seraient entreprises pour remettre rapidement le système en état de fonctionnement et minimiser l'impact sur le service?\n",
      "\n",
      "2. Imaginons une fissure détectée sur le châssis d'une automotrice.  Quelles seraient les étapes de la maintenance corrective, de l'évaluation des dommages à la réparation finale, et comment s'assurerait-on de la sécurité du matériel roulant après l'intervention?\n",
      "\n",
      "3.  En cas de défaillance soudaine du système de freinage d'un tramway en pleine ligne, quelles procédures de maintenance corrective seraient mises en place en urgence pour garantir la sécurité des passagers et la reprise rapide du service?\n",
      "\n",
      "Réponse : Pour le matériel roulant autre que les wagons, la maintenance corrective peut inclure des travaux suite à des déprédations intérieures/extérieures, des opérations saisonnières (climatisation, chauffage, dispositifs d'adhérence), le relevé dimensionnel des essieux et l'échange de PRM au potentiel.  FRET SNCF propose ces services de maintenance corrective.\n",
      "\n",
      "Source: data/doc.pdf\n",
      "Page: 14\n",
      "Extrait: DOCUMENT DE REFERENCE DE MAINTENANCE DE FRET SNCF POUR L‘HORAIRE DE SERVICE 2024 \n",
      " \n",
      "Page 15 \n",
      " \n",
      "o L’examen rapide des essieux ; des bogies  ; des équipements en cabine et des aménagements \n",
      "intérieurs ;...\n"
     ]
    }
   ],
   "source": [
    "# Utilisation\n",
    "query = \"Pour les Matériels Roulants autres que les wagons, en quoi consiste les opérations de maintenance corrective?\"\n",
    "result = advanced_rag_chain(query)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
